---
title: "text_EDA"
author: "Alex MacCalman"
date: "11/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(LaF)
library(tidytext)
library(stringr)
library(sentimentr)
library(tidyverse)
library(tm)
library(wordcloud)
```
## Import examine data.  
First we will import all the data and examine the file size and the number of lines in each data file.  
```{r}
twit <- as_tibble(read_lines("en_US.twitter.txt"))
blog <- as_tibble(read_lines("en_US.blogs.txt"))
news <- as_tibble(read_lines("en_US.news.txt"))

file_list <- list.files(pattern = ".txt$")

file.size(file_list[1])
file_list %>% map_dbl(~file.size(.)/1000000) # calculate MB size

rows <- c(nrow(twit), nrow(blog), nrow(news))

table <- tibble(file_name = file_list, 
             size_MBs = file_list %>% map_dbl(~file.size(.)/1000000),
             num_lines = rows)

# max_words_n_line = max(str_count(twit, "\\S+")),
# max_char_in_lines = max(nchar(twit)))

```
Because the data files are so large we will sample a portion of the data.  
## Sample a collection of the data    
```{r}
set.seed(123)
twit <- as_tibble(sample_lines("en_US.twitter.txt", 5000)) # sample a set of lines to work with
blog <- as_tibble(sample_lines("en_US.blogs.txt", 5000))
news <- as_tibble(sample_lines("en_US.news.txt", 5000))
# all <- list(c(blog, news, twit))

corpus <- rbind(twit, blog, news)
gc() # garge clean to free up memory
```
To create a corpus, we will perform the follwoing cleaning steps:
1. token into sentences using tidytext package, strip punctuation, and cast all word to lower case.  
2. remove all non-ASCII characters.
3. merge multiple spaces to a single space and remove trailing/leading spaces.
4. remove URLs.
5. eliminate numbers.
6. add STAR T and END words to each sentance.
7. remove profanity words.  
## Clean the corpus  
```{r}
# tokenizes into sentences, stips punctuation, converts to lowercase
corpus <- corpus %>% 
  unnest_tokens(sentance, value, token = "sentences", to_lower = TRUE, strip_punct = TRUE)
# remove non-ascii characters
corpus$sentance <- gsub("[^\x20-\x7E]", "", corpus$sentance)
# merge multiple spaces to a single space and remove trailing/leading spaces
corpus$sentance <- str_replace(gsub("\\s+", " ", str_trim(corpus$sentance)), "B", "b")
# removes URLs
corpus$sentance <- gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", corpus$sentance)
# eliminate numbers
corpus$sentance <- gsub('[[:digit:]]+', '', corpus$sentance)
# add START and END words to each sentence
corpus$sentance <- paste0("START", " ", corpus$sentance, " ", "END")

# create a token list
tokens <- corpus %>% 
  unnest_tokens(word, sentance, to_lower = FALSE)

# eliminate the profanity words
# download Google's bad word list 
bad_words <- read_tsv("https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/badwordslist/badwords.txt", col_names = FALSE)
bad_words <- as_tibble(bad_words) %>% 
  rename(word = X1)
tokens <- tokens %>%
  anti_join(bad_words)

```
## Create n-grams and save a list of ngrams 
We now will create unigrams, bigrams and trigrams.  
```{r}
unigram <- tokens %>% 
  count(word, sort = TRUE)

bigram <- tokens %>% 
  unnest_tokens(bigram, word, token = "ngrams", n = 2, to_lower = FALSE) %>% 
  count(bigram, sort = TRUE) %>%
  filter(n > 4) %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

trigram <- tokens %>% 
  unnest_tokens(trigram, word, token = "ngrams", n = 3, to_lower = FALSE) %>% 
  count(trigram, sort = TRUE) %>% 
  filter(n > 4) %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ")


```
Now we will show the n-gram distributions and word clouds.  
```{r}
# select top 50 unigrams
top_unigrams <- unigram %>% 
  filter(word != "END", word != "START") %>% 
  head(50)
# display histogram
top_unigrams %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
# create unigram word cloud
 wordcloud(top_unigrams$word, top_unigrams$n, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2"))


# select top 50 bigrams
top_bigrams <- bigram %>% 
  filter(word1 != "END", word1 != "START",
         word2 != "END", word2 != "START") %>%
  unite(bigram, word1, word2, sep = " ") %>% 
  head(50)
# display histogram
top_bigrams %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(bigram, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

# create unigram word cloud
 wordcloud(top_bigrams$bigram, top_bigrams$n, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2"))

# select top 50 trigrams
top_trigrams <- trigram %>% 
  filter(word1 != "END", word1 != "START",
         word2 != "END", word2 != "START",
         word3 != "END", word3 != "START") %>% 
  unite(trigram, word1, word2, word3, sep = " ") %>% 
  head(50)
# display histogram
top_trigrams %>%
  mutate(trigram = reorder(trigram, n)) %>%
  ggplot(aes(trigram, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

# create unigram word cloud
 wordcloud(top_trigrams$trigram, top_trigrams$n, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2"))
```

## Find the minimum number of words to achieve 50% and 90% coverage.  
```{r}
# calculate the cumulative sum of each word divided by the total sum of all words
coverage <- tibble(coverage = cumsum(unigram$n) / sum(unigram$n) * 100,
                   words = 1:nrow(unigram))
# plot the coverage
coverage %>% 
  ggplot(aes(x = words, y = coverage)) +
  geom_line() +
  ggtitle("Word Coverage vs. Top Set of words")

# find the minimum number of words that will achieve 50% coverage
coverage50percent <- min(coverage[coverage$coverage > 50, ]$words)
coverage90percent <- min(coverage[coverage$coverage > 90, ]$words)
total_words <- tail(coverage$words, 1)
```
## How to increase coverage. 
We would need `r coverage50percent` words to achcieve 50% coverage or the minimum number of words added that would represent half of the words present. `r coverage90percent` words would achieve 90%.  

## How do you evaluate how many of the words come from foreign languages.  
We can find foriegn language words in our corpus by creating a list of word to perform an semi-join on our unigram table to find what words are in the foreign language table. 

## Modeling goals.  
Our goal is to build a Kats back off model with trigrams, bigrams, and unigrams to allow for theability to handle unknown words. The Shiy app will prompt the user to type in words. The application will extract the last tow words and execute teh katz backoff model to predict the next word.  






