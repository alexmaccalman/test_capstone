---
title: "text_EDA"
author: "Alex MacCalman"
date: "11/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(LaF)
library(tidytext)
library(stringr)
library(sentimentr)
library(tidyverse)
library(tm)
library(wordcloud)
```
## Import examine data.  
First we will import all the data and examine the file size and the number of lines in each data file.  
```{r}
twit <- as_tibble(read_lines("en_US.twitter.txt"))
blog <- as_tibble(read_lines("en_US.blogs.txt"))
news <- as_tibble(read_lines("en_US.news.txt"))

file_list <- list.files(pattern = ".txt$")

file.size(file_list[1])
file_list %>% map_dbl(~file.size(.)/1000000) # calculate MB size

rows <- c(nrow(twit), nrow(blog), nrow(news))

table <- tibble(file_name = file_list, 
             size_MBs = file_list %>% map_dbl(~file.size(.)/1000000),
             num_lines = rows)

# max_words_n_line = max(str_count(twit, "\\S+")),
# max_char_in_lines = max(nchar(twit)))

```
Because the data files are so large we will sample a portion of the data.  
## Sample a collection of the data    
```{r}
set.seed(123)
twit <- as_tibble(sample_lines("en_US.twitter.txt", 10000)) # sample a set of lines to work with
blog <- as_tibble(sample_lines("en_US.blogs.txt", 10000))
news <- as_tibble(sample_lines("en_US.news.txt", 10000))
# all <- list(c(blog, news, twit))

corpus <- rbind(twit, blog, news)
gc() # garge clean to free up memory
```
To create a corpus, we will perform the follwoing cleaning steps:
1. token into sentences using tidytext package, strip punctuation, and cast all word to lower case.  
2. remove all non-ASCII characters.
3. merge multiple spaces to a single space and remove trailing/leading spaces.
4. remove URLs.
5. eliminate numbers.
6. add STAR T and END words to each sentance.
7. remove profanity words.  
## Clean the corpus  
```{r}
# tokenizes into sentences, stips punctuation, converts to lowercase
corpus <- corpus %>% 
  unnest_tokens(sentance, value, token = "sentences", to_lower = TRUE, strip_punct = TRUE)
# remove non-ascii characters
corpus$sentance <- gsub("[^\x20-\x7E]", "", corpus$sentance)
# merge multiple spaces to a single space and remove trailing/leading spaces
corpus$sentance <- str_replace(gsub("\\s+", " ", str_trim(corpus$sentance)), "B", "b")
# removes URLs
corpus$sentance <- gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", corpus$sentance)
# eliminate numbers
corpus$sentance <- gsub('[[:digit:]]+', '', corpus$sentance)
# add START and END words to each sentence
corpus$sentance <- paste0("START", " ", corpus$sentance, " ", "END")

# create a token list
tokens <- corpus %>% 
  unnest_tokens(word, sentance, to_lower = FALSE)

# eliminate the profanity words
# download Google's bad word list 
bad_words <- read_tsv("https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/badwordslist/badwords.txt", col_names = FALSE)
bad_words <- as_tibble(bad_words) %>% 
  rename(word = X1)
tokens <- tokens %>%
  anti_join(bad_words)

```
## Create n-grams and save a list of ngrams 
We now will create unigrams, bigrams and trigrams.  
```{r}
unigram <- tokens %>% 
  count(word, sort = TRUE)

bigram <- tokens %>% 
  unnest_tokens(bigram, word, token = "ngrams", n = 2, to_lower = FALSE) %>% 
  count(bigram, sort = TRUE) %>%
  filter(n > 4) %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

trigram <- tokens %>% 
  unnest_tokens(trigram, word, token = "ngrams", n = 3, to_lower = FALSE) %>% 
  count(trigram, sort = TRUE) %>% 
  filter(n > 4) %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ")


```
Now we will show the n-gram distributions and word clouds.  
```{r}
# select top 50 unigrams
top_unigrams <- unigram %>% 
  filter(word != "END", word != "START") %>% 
  head(50)
# display histogram
top_unigrams %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
# create unigram word cloud
 wordcloud(top_unigrams$word, top_unigrams$n, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2"))


# select top 50 bigrams
top_bigrams <- bigram %>% 
  filter(word1 != "END", word1 != "START",
         word2 != "END", word2 != "START") %>%
  unite(bigram, word1, word2, sep = " ") %>% 
  head(50)
# display histogram
top_bigrams %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(bigram, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

# create unigram word cloud
 wordcloud(top_bigrams$bigram, top_bigrams$n, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2"))

# select top 50 trigrams
top_trigrams <- trigram %>% 
  filter(word1 != "END", word1 != "START",
         word2 != "END", word2 != "START",
         word3 != "END", word3 != "START") %>% 
  unite(trigram, word1, word2, word3, sep = " ") %>% 
  head(50)
# display histogram
top_trigrams %>%
  mutate(trigram = reorder(trigram, n)) %>%
  ggplot(aes(trigram, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

# create unigram word cloud
 wordcloud(top_trigrams$trigram, top_trigrams$n, scale = c(4, 0.5), colors = brewer.pal(8, "Dark2"))
```

## Find the minimum number of words to achieve 50% and 90% coverage.  
```{r}
# calculate the cumulative sum of each word divided by the total sum of all words
coverage <- tibble(coverage = cumsum(unigram$n) / sum(unigram$n) * 100,
                   words = 1:nrow(unigram))
# plot the coverage
coverage %>% 
  ggplot(aes(x = words, y = coverage)) +
  geom_line() +
  ggtitle("Word Coverage vs. Top Set of words")

# find the minimum number of words that will achieve 50% coverage
coverage50percent <- min(coverage[coverage$coverage > 50, ]$words)
coverage90percent <- min(coverage[coverage$coverage > 90, ]$words)
total_words <- tail(coverage$words, 1)
```
## How to increase coverage. 
We would need `r coverage50percent` words to achcieve 50% coverage or the minimum number of words added that would represent half of the words present. `r coverage90percent` words would achieve 90%.  

## How do you evaluate how many of the words come from foreign languages.  
We can find foriegn language words in our corpus by creating a list of word to perform an semi-join on our unigram table to find what words are in the foreign language table. 

## Modeling goals.  
Our goal is to build a Kats back off model with trigrams, bigrams, and unigrams to allow for theability to handle unknown words. The Shiy app will prompt the user to type in words. The application will extract the last tow words and execute teh katz backoff model to predict the next word.  

Test model  
```{r}


trigram_fun <- function(pre_word_df, discount1, discount2){
  
  # 3 calculate probabilities of words completing observed trigrams
  
  # count of bigram word
  count_word <- bigram %>% 
    filter(word1 == pre_word_df$word1 & word2 == pre_word_df$word2)
  bigram_count <- count_word$n
  
  # create the observed trigram table
  obs_tri <- trigram %>% 
    filter(word1 == pre_word_df$word1 & word2 == pre_word_df$word2)  %>% 
    mutate(prob = (n - discount1) / bigram_count)
  
  # 4 calculate probabilities of words completing unobserved trigrams
  
  # 4.1 Find the words that complete the unobserved trigrams
  obs_tri_tails <- obs_tri$word3 # grabs the last word
  unobs_tri_tails <- unigram[!(unigram$word %in% obs_tri_tails),]$word
  
  
  # 4.2 Calculate discounted probability mass at the bigram level, alpha1
  
  pre_word2 <- pre_word_df$word2 # second word of the bigram prefix
  pre_word2_df <- unigram[unigram$word == pre_word2,] # single row of word and its count
  
  pre_word_bigram <- bigram[bigram$word1 == pre_word2,] # get all bigrams that start with the pre-word (unigram)
  
  # calcualte the alpha value
  alpha_bi <- 0
  if (nrow(pre_word_bigram) > 0){
    alpha_bi <- 1 - (sum(pre_word_bigram$n - discount1) / pre_word2_df$n)
  }
  
  # 4.3 Calculate the back off probability for bigrams
  
  pre_word2 # second word of the bigram prefix
  unobs_tri_tails # the words that complete the unobserved trigrams
  
  bo_bigrams <- paste(pre_word2, unobs_tri_tails, sep = " ") # get back off bigrams
  
  # unite the bigrams so there is one bigram column
  bigrams_united <- bigram %>% 
    unite(bigram, word1, word2, sep = " ")
  # get the observed back off bigrams
  obs_bo_bigrams <- bigram[bigrams_united$bigram %in% bo_bigrams, ]
  
  # unite the obs_bo_bigrams so there is one bigram column
  obs_bo_bigrams_united <- obs_bo_bigrams %>% 
    unite(bigram, word1, word2, sep = " ")
  
  # get the unobserved back off bigrams
  unobs_bo_bigrams <- bo_bigrams[!(bo_bigrams %in% obs_bo_bigrams_united$bigram)]
  
  # calculate the observed bigram probability
  first_word <- obs_bo_bigrams$word1
  first_word_n <- unigram[unigram$word %in% first_word, ]
  # calcualte the observed bigram probability
  prob_obs_bigram <- (obs_bo_bigrams$n - discount2) / first_word_n$n
  # create a table with bigram and prop
  prob_obs_bigram <- data.frame(bigram = obs_bo_bigrams_united$bigram, prob = prob_obs_bigram)
  
  # distribute discounted bigram probability mass to unobserved bigrams in proportion to unigram
  
  # get the unobserved bigram tails
  prob_unobs_bigram <- str_split_fixed(unobs_bo_bigrams, " ", 2)[ ,2]
  # make a table of unobserved tails
  prob_unobs_bigram <- unigram[unigram$word %in% prob_unobs_bigram, ]
  # convert counts to probabilities
  prob_unobs_bigram <- data.frame(bigram = unobs_bo_bigrams,
                                  prob = alpha_bi * prob_unobs_bigram$n / sum(prob_unobs_bigram$n))
  # add the prop observed bigram to data frame
  prob_bigram <- rbind(prob_obs_bigram, prob_unobs_bigram)
  
  
  #check 
  unobs <- prob_bigram[-1,] # take away the observed bigram
  sum(unobs$prob)
  
  # 4.4 Calculate discout probability mass at the trigram level
  
  ## Calculate the total probability mass discounted from all observed trigrams.
  ## This is the amount of probability mass which is
  ## redistributed to UNOBSERVED trigrams. If no trigrams starting with the bigram
  ## exist, alpha_tri = 1 
  
  big <- bigram %>% 
    filter(bigram$word1 == pre_word_df$word1 & bigram$word2 == pre_word_df$word2)
  
  
  alpha_tri <- 1
  if (nrow(obs_tri) > 0){
    alpha_tri <- 1 - (sum(obs_tri$n - discount2) / big$n[1])
  }
  
  # 4.5 Calculate unobserved trigram probabilities
  
  prob_bigram <- prob_bigram %>% 
    arrange(desc(prob_bigram$prob)) # sort the table
  
  sum_prob_bo_bigrams <- sum(prob_bigram$prob)
  first_pre_word <- pre_word_df$word1
  unobs_tri <- paste(first_pre_word, prob_bigram$bigram, sep = " ")
  prob_unobs_tri <- alpha_tri * prob_bigram$prob / sum_prob_bo_bigrams
  unobs_tri_df <- data.frame(trigram = unobs_tri, prob = prob_unobs_tri)
  
  # 5 Select word with the highest probability
  # unite this and rbind with prob_unobs_tri
  obs_tri 
  obs_tri_df <- obs_tri %>% 
    unite(trigram, word1, word2, word3, sep = " ") %>% 
    select(trigram, prob)
  
  prob_trigram <- rbind(obs_tri_df, unobs_tri_df)
  prob_trigram
}

# 1.2 Select bigram and trigram discounts
discount1 <- 0.5
discount2 <- 0.5
# 2 Select bigram prefix of word to be predicted

phrase_input <- "and I'd" 
phrase_input <- str_trim(phrase_input, side = "both") #trim white spaces on both sides

if (str_count(phrase_input, "\\w+") <= 1){
  phrase_input <- paste0("notAword", " ", phrase_input)
}

pre_words <- word(phrase_input, -2, -1) # grab trhe last two word in the input phrase

#create tibble for word input
pre_word_df <- tibble(line = 1, text = pre_words)%>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

top_amount <- 100
# is the last word in the unigram model?
if (sum(unigram$word %>% str_detect(pre_word_df$word2)) > 0){ 
  result <- trigram_fun(pre_word_df, discount1, discount2) %>% 
    separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% 
    arrange(desc(prob)) %>% 
    select(word3, prob) %>% 
    rename(word = word3) %>% 
    filter(word != "END", word != "START") %>% 
    head(top_amount)
}else{
  result <- unigram %>% 
    filter(word != "END", word != "START") %>% 
    mutate(prob = n / sum(n)) %>% 
    select(word, prob) %>% 
    arrange(desc(prob)) %>% 
    head(top_amount)
}
result
```

try this
https://rstudio-pubs-static.s3.amazonaws.com/363937_d251370b9d494974bc6875a99bab1ce9.html






